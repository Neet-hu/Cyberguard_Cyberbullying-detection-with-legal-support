{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from tensorflow.keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = \"Dataset for Detection of Cyber-Trolls.json\"\n",
    "csvpath =  \"CyberBullyingTypesDataset.csv\"  \n",
    "pickle_model_path = \"BI LSTM/models/model.pkl\"\n",
    "keras_model_path = \"BI LSTM/models/model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text) \n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  \n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(jsonpath, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data.append(json.loads(line))  \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line}\") \n",
    "            continue\n",
    "\n",
    "json_texts = [item['content'] for item in data]\n",
    "json_labels = [item['annotation']['label'][0] for item in data]  \n",
    "\n",
    "df = pd.read_csv(csvpath)\n",
    "\n",
    "csv_texts = df['Tweet'].astype(str).tolist()  \n",
    "csv_labels = df['Class'].astype(str).tolist()  \n",
    "\n",
    "combined_texts = json_texts + csv_texts\n",
    "combined_labels = json_labels + csv_labels\n",
    "\n",
    "# combined_texts = json_texts \n",
    "# combined_labels = json_labels \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "texts = [clean_text(text) for text in combined_texts]\n",
    "\n",
    "\n",
    "\n",
    "cleaned_df = pd.DataFrame({'texts': texts, 'label': combined_labels})\n",
    "cleaned_df['label'] = cleaned_df['label'].str.strip().str.lower()  \n",
    "cleaned_df['label'] = cleaned_df['label'].apply(lambda x: 1 if x != '0' else 0)\n",
    "\n",
    "\n",
    "cleaned_df = cleaned_df.dropna(subset=['texts'])  \n",
    "cleaned_df = cleaned_df[cleaned_df['texts'].str.strip() != '']  \n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=['texts']) \n",
    "cleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully with cleaned data.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "with open('8000other.txt', 'r', encoding='utf-8') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "cleaned_sentences = [clean_text(sentence) for sentence in sentences]\n",
    "\n",
    "with open('bullying_data3.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['text', 'label'])  \n",
    "\n",
    "    for sentence in cleaned_sentences:\n",
    "        if sentence:  \n",
    "            writer.writerow([sentence, 0])\n",
    "\n",
    "print(\"CSV file created successfully with cleaned data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 45405\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "def clean_and_split_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    words = text.lower().split() \n",
    "    return words\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "texts = df['texts'].astype(str).tolist()\n",
    "all_words = []\n",
    "for text in texts:\n",
    "    all_words.extend(clean_and_split_text(text))\n",
    "unique_words = set(all_words)\n",
    "num_unique_words = len(unique_words)\n",
    "print(f\"Number of unique words: {num_unique_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
